{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82151150-9d2b-4f77-8123-8b82340cdf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torchvision import models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "401b96b2-c9b6-4459-a8b4-ae95a2da14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights of the model and add nn.LogSoftmax(dim=1) to the end\n",
    "model_name = 'resnet18'\n",
    "model = models.resnet18(pretrained=True)\n",
    "model = nn.Sequential(model, nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47625639-e733-4d56-9fb8-92c4b806f0d9",
   "metadata": {},
   "source": [
    "## Convert the last FC layer to a 1x1 convolution\n",
    "Now, let us derive a ResNet18 model where we convert the last FC layer to a 1x1 convolution and skip the GAP layer\n",
    "- Feature embedding `512`\n",
    "- Number of classes: `1000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04f7f6b0-de7a-4f99-ae0d-f0d2c8860cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res18_model = models.resnet18(pretrained=True)\n",
    "fc = res18_model.fc.state_dict()\n",
    "\n",
    "in_ch = 512\n",
    "out_ch = fc[\"weight\"].size(0)\n",
    "finalConv = nn.Conv2d(in_ch, out_ch, 1, 1)\n",
    "\n",
    "### get the weights from the fc layer\n",
    "finalConv.load_state_dict({\"weight\":fc[\"weight\"].view(out_ch, in_ch, 1, 1), \"bias\":fc[\"bias\"]})\n",
    "res18_conv = nn.Sequential(*list(res18_model.children())[:-2]+[finalConv])\n",
    "\n",
    "res18_model.eval()\n",
    "res18_conv.eval()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a303d891-3450-413f-9aa2-f7102056191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(fc))\n",
    "\n",
    "fc['weight'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0337a24-c63d-4d3c-aa0c-55d4de80917c",
   "metadata": {},
   "source": [
    "## RetFound\n",
    "- patch size: `16x16`\n",
    "- without overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf2b22-ab94-4825-861b-562b3bb2b37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73737984-d813-4494-b998-07043b193863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.pyenv/versions/miniconda3-latest/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model from hugging face\n",
    "model = timm.create_model(\"hf_hub:bitfount/RETFound_MAE\", pretrained=True)\n",
    "\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e0eab30-84b3-48f8-9135-72fad846ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_img = torch.randn((1, 3, 224, 224), device='cuda')\n",
    "out = model(ts_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add34146-f5f6-41e5-8294-aed0cdb37404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e02b3c-c9a6-42b9-a22c-26272531175f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f50adf6-d6ef-4142-8b5f-93608a8d5415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('patch_embed',\n",
       "  PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )),\n",
       " ('pos_drop', Dropout(p=0.0, inplace=False)),\n",
       " ('patch_drop', Identity())]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = list(model.named_children()) # generator\n",
    "print(len(ll))\n",
    "ll[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "429eba6c-e177-4d9f-9cab-74b6d9f289f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block(\n",
       "  (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (attn): Attention(\n",
       "    (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "    (q_norm): Identity()\n",
       "    (k_norm): Identity()\n",
       "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (ls1): Identity()\n",
       "  (drop_path1): Identity()\n",
       "  (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): Mlp(\n",
       "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (drop1): Dropout(p=0.0, inplace=False)\n",
       "    (norm): Identity()\n",
       "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (drop2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (ls2): Identity()\n",
       "  (drop_path2): Identity()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7284a60d-b109-4831-8196-aa7a9be37c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=3072, bias=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0].attn.qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47e29132-eede-4657-8c77-1ebda0a9b01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 3072\n"
     ]
    }
   ],
   "source": [
    "in_ch = model.blocks[0].attn.qkv.in_features\n",
    "out_ch = model.blocks[0].attn.qkv.out_features\n",
    "\n",
    "qkv = model.blocks[0].attn.qkv.state_dict()\n",
    "\n",
    "conv_qkv = nn.Conv2d(in_ch, out_ch, 1, 1)\n",
    "\n",
    "conv_qkv.load_state_dict({\"weight\":qkv[\"weight\"].view(out_ch, in_ch, 1, 1), \"bias\":qkv[\"bias\"]}) # init\n",
    "\n",
    "model.blocks[0].attn.qkv = conv_qkv\n",
    "\n",
    "print(in_ch, out_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437cb9f3-a3f4-496b-8523-9b3fa3a1126e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92d38716-1338-4753-8c9d-6d65c351fc1e",
   "metadata": {},
   "source": [
    "## Explanable RetFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ddf2ae25-df89-41b8-9952-c83986d71109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model from hugging face\n",
    "model = timm.create_model(\"hf_hub:bitfount/RETFound_MAE\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f77be-3dee-4c6d-846e-154f9e74b6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "435039cc-2797-472c-860f-3658d39d1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {}\n",
    "\n",
    "for name, layer in model.named_children():\n",
    "    layers[name] = layer\n",
    "\n",
    "layers_before = {}\n",
    "layer_name_before = ['patch_embed', 'pos_drop', 'patch_drop', 'norm_pre']\n",
    "for name in layer_name_before:\n",
    "    layers_before[name] = layers[name]\n",
    "\n",
    "layers_after = {}\n",
    "layer_name_after= ['norm', 'fc_norm', 'head_drop']\n",
    "for name in layer_name_after:\n",
    "    layers_after[name] = layers[name]\n",
    "\n",
    "\n",
    "#classification head\n",
    "n_classes = 5\n",
    "in_ftrs = model.head.in_features\n",
    "classifier_ = nn.Conv2d(in_channels=in_ftrs, out_channels=n_classes, kernel_size=1)\n",
    "classifier = {'classifier': classifier_}\n",
    "\n",
    "#layers_before = OrderedDict(layers_before)\n",
    "#self_layers_before = nn.Sequential(layers_before)\n",
    "\n",
    "#self_layers_before\n",
    "#layers_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d3c4875d-26c5-487b-8e5d-9f41e78ef75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['patch_embed', 'pos_drop', 'patch_drop', 'norm_pre', 'blocks', 'norm', 'fc_norm', 'head_drop', 'head'])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cfdcb-72ef-4968-af84-30a92c9f0c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a6afaacf-2f79-4875-8f28-1f03fe2b93da",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_block = layers['blocks']\n",
    "\n",
    "for idx, (name, layer) in enumerate(main_block.named_children()):\n",
    "    #print(f'{name}, \\n {layer}')\n",
    "    \n",
    "    for name_, layer_ in layer.named_children():\n",
    "        #print(f'Name: \\t {name_}, \\n {layer_}')\n",
    "        \n",
    "        if (name_ == 'attn') or (name_ == 'mlp'):\n",
    "            \n",
    "            for name__, layer__ in layer_.named_children():\n",
    "                #print(f'Name 2 : \\t {name__}, \\n {layer__}')\n",
    "\n",
    "                if isinstance(layer__, nn.Linear): \n",
    "                    in_ftrs = layer__.in_features\n",
    "                    out_ftrs = layer__.out_features\n",
    "                    \n",
    "                    # Create a 1x1 convolutional layer\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_ftrs, out_channels=out_ftrs, kernel_size=1)\n",
    "\n",
    "                    weight_bias = layer__.state_dict()\n",
    "                    conv_layer.load_state_dict({\"weight\": weight_bias[\"weight\"].view(out_ftrs, in_ftrs, 1, 1), \"bias\": weight_bias[\"bias\"]})\n",
    "                    #print(main_block[idx].attn.qkv)\n",
    "\n",
    "                    # dynamic assignment\n",
    "                    setattr(getattr(main_block[idx], name_), name__, conv_layer)\n",
    "                    \n",
    "                    #if name__ == 'qkv':\n",
    "                    #    main_block[idx].attn.qkv = conv_layer\n",
    "\n",
    "                    #if name__ == 'proj':\n",
    "                    #    main_block[idx].name_.proj = conv_layer\n",
    "                    \n",
    "                    #print(f'nn.linear: {name__} \\t {in_ftrs}, \\t {out_ftrs}')\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a901b391-bf00-4835-90ed-dfe627d6147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patch_embed': PatchEmbed(\n",
       "   (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "   (norm): Identity()\n",
       " ),\n",
       " 'pos_drop': Dropout(p=0.0, inplace=False),\n",
       " 'patch_drop': Identity(),\n",
       " 'norm_pre': Identity()}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "00c46302-06d3-461e-afc8-281df98bd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_block = {}\n",
    "\n",
    "for name, layer in main_block.named_children():\n",
    "    layers_block[name] = layer\n",
    "\n",
    "layers_block_ = OrderedDict(layers_block)\n",
    "layers_block_ = nn.Sequential(layers_block_)\n",
    "layers_block_ = {'blocks': layers_block_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "061c076f-858f-4f84-98ab-425816f839d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "01de5407-a7b9-49b0-95d8-efc99de6312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers_block_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e358ef-dd0c-4b82-a6e4-4caa47745c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "94f1a9a3-2512-475f-8f07-f17f16d39577",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_layers = OrderedDict(list(layers_before.items()) + list(layers_block_.items()) + list(layers_after.items()) + list(classifier.items())) \n",
    "\n",
    "new_model = nn.Sequential(merged_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa42af9-ea94-4a6a-8108-a06ec7d3a335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7445d1e-b40d-4191-a057-26da9fd55adc",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1e2550-e095-4474-85e8-25622bfea3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torchvision import models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6045719a-e34c-423d-abb8-8b28da7d75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainRetFound(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ExplainRetFound, self).__init__()  \n",
    "        self.size = input_size//16, input_size//16\n",
    "        \n",
    "        # Load pre-trained model from hugging face\n",
    "        model = timm.create_model(\"hf_hub:bitfount/RETFound_MAE\", pretrained=True)\n",
    "\n",
    "        layers_before, layers_after = self.get_layer_before_after(model)\n",
    "        main_block_layer = self.from_linear_to_conv_layers(model)\n",
    "        \n",
    "        in_ftrs = model.head.in_features\n",
    "        cls_head = nn.Conv2d(in_channels=in_ftrs, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "        self.norm = nn.GroupNorm(1, 1024, eps=1e-06) # default = eps=1e-05\n",
    "        layers_after['norm'] = self.norm\n",
    "\n",
    "        self.layers_before = nn.Sequential(OrderedDict(layers_before))\n",
    "        self.main_block_layer = nn.Sequential(OrderedDict(main_block_layer)) \n",
    "        self.layers_after = nn.Sequential(OrderedDict(layers_after))\n",
    "        self.classifier = cls_head\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=self.size, stride=(1,1), padding=0) \n",
    "\n",
    "    def get_layer_before_after(self, model):\n",
    "        layers = {}   # all layers\n",
    "        for name, layer in model.named_children():\n",
    "            layers[name] = layer\n",
    "            \n",
    "        layers_before = {}\n",
    "        layer_name_before = ['patch_embed', 'pos_drop', 'patch_drop', 'norm_pre']\n",
    "        for name in layer_name_before:\n",
    "            layers_before[name] = layers[name]\n",
    "\n",
    "        layers_after = {}\n",
    "        layer_name_after= ['norm', 'fc_norm', 'head_drop']\n",
    "        for name in layer_name_after:\n",
    "            layers_after[name] = layers[name]\n",
    "\n",
    "        return layers_before, layers_after\n",
    "\n",
    "    def from_linear_to_conv_layers(self, model):\n",
    "        blocks = model.blocks\n",
    "        #print(blocks)\n",
    "\n",
    "        for idx, (name, layer) in enumerate(blocks.named_children()):\n",
    "            #print(f'idx \\t {idx} \\t {name} \\n {layer}')\n",
    "            for name_, layer_ in layer.named_children():\n",
    "                if (name_ == 'attn') or (name_ == 'mlp'):\n",
    "                    for name__, layer__ in layer_.named_children():\n",
    "                        if isinstance(layer__, nn.Linear):\n",
    "                            in_ftrs = layer__.in_features\n",
    "                            out_ftrs = layer__.out_features\n",
    "                            \n",
    "                            # Create a 1x1 convolutional layer\n",
    "                            conv_layer = nn.Conv2d(in_channels=in_ftrs, out_channels=out_ftrs, kernel_size=1)\n",
    "                            weight_bias = layer__.state_dict()\n",
    "                            conv_layer.load_state_dict({\"weight\": weight_bias[\"weight\"].view(out_ftrs, in_ftrs, 1, 1), \n",
    "                                                        \"bias\": weight_bias[\"bias\"]})\n",
    "\n",
    "                            # dynamic assignment\n",
    "                            setattr(getattr(blocks[idx], name_), name__, conv_layer)\n",
    "\n",
    "        layers_block = {}\n",
    "        for name, layer in blocks.named_children():\n",
    "            layers_block[name] = layer\n",
    "            \n",
    "        return layers_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = self.size\n",
    "    \n",
    "        # Initial processing\n",
    "        x = self.layers_before(x)\n",
    "        bs, hw, c = x.shape  # Assuming input shape is (batch_size, height * width, channels)\n",
    "    \n",
    "        # Reshape before block processing\n",
    "        x = x.view(bs, h, w, c).permute(0, 3, 1, 2)  # (bs, c, h, w)\n",
    "        \n",
    "        # Loop through all 24 blocks\n",
    "        for idx, block in enumerate(self.main_block_layer):\n",
    "            x = self.norm(x)  # Normalization\n",
    "            \n",
    "            #print(f'idx: {idx}, \\t {x.shape}')\n",
    "            # Attention mechanism\n",
    "            x = block.attn.qkv(x) \n",
    "            x = block.attn.q_norm(x)\n",
    "            x = block.attn.k_norm(x)\n",
    "            x = x.view(bs, 1024, 3, h, w).sum(dim=2)  # Sum over QKV\n",
    "            x = block.attn.proj(x)\n",
    "            x = block.attn.proj_drop(x)\n",
    "    \n",
    "            # Skip connection & dropout\n",
    "            x = block.ls1(x)\n",
    "            x = block.drop_path1(x)\n",
    "    \n",
    "            # Feedforward MLP block\n",
    "            x = self.norm(x) \n",
    "            x = block.mlp.fc1(x)\n",
    "            x = block.mlp.act(x)\n",
    "            x = block.mlp.drop1(x)\n",
    "            x = block.mlp.norm(x)\n",
    "            x = block.mlp.fc2(x)\n",
    "            x = block.mlp.drop2(x)\n",
    "    \n",
    "            # Skip connection & dropout\n",
    "            x = block.ls2(x)\n",
    "            x = block.drop_path2(x)\n",
    "\n",
    "        x = self.layers_after(x)\n",
    "        activation = self.classifier(x)\n",
    "        out = self.avgpool(activation) \n",
    "        out = out.view(out.shape[0], -1)    # (bs, n_class)\n",
    "        \n",
    "        return out, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c58b9ca-5290-4f91-82f4-212e1e7777ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_model = ExplainRetFound(input_size=224, num_classes=5)\n",
    "tmp_model.to('cuda')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09c76be5-a57e-4f52-b708-ce891d06f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_model.layers_before\n",
    "#tmp_model.layers_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59fd03-5f88-450e-b89f-d9a607a89d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2aed3ce3-0cb8-411a-bce1-1cc42f67938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_img = torch.randn((1, 3, 224, 224), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30fcc00b-54f2-4217-a995-3f0693f21b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5]), torch.Size([1, 5, 14, 14]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, acts = tmp_model(ts_img)\n",
    "out.shape, acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78d4ce2f-e735-4f55-8088-4969999cb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = tmp_model.main_block_layer\n",
    "#aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708442e-9855-402f-98dd-4bbe9ca4b233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9b022-d2b1-43c3-a91c-c20c2070f1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "464dc2c5-14a3-4aa2-8ce3-2086946d30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"hf_hub:bitfount/RETFound_MAE\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3f559d2-637f-4a14-bfd3-44c0bc415e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3072/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6658ef79-9825-42ba-bcd8-230382b9a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3aecb-76dc-42e7-bad8-769f237c606c",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e34f1-49cc-4b64-ac58-ea4b72a1d2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf8ad59-12bb-4147-ae37-75aecea9ef81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94030d3a-fb7c-4c1f-86ef-5e13e004206b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ts_img = torch.randn((1, 3, 224, 224), device='cuda')\n",
    "\n",
    "new_model.to('cuda')\n",
    "new_model.eval()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fc70977-dbad-4530-844f-7df913e0c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = new_model(ts_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219dab2-7f9e-4434-888b-14ca7ec41014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8ba82000-b2a2-48a4-881c-04ab72343e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers_block['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5893eea7-5788-4b38-8dca-6214669339a0",
   "metadata": {},
   "source": [
    "## Test debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16c8b0eb-97fc-4ebb-83c7-df9528455533",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers1 = {'0': layers_block['0']}\n",
    "layers1_ = {'blocks': nn.Sequential(OrderedDict(layers1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5138c362-c2a2-4a07-89a9-7d564f81d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_layers = OrderedDict(list(layers_before.items()) ) #+ list(layers1_.items())\n",
    "test_model = nn.Sequential(test_layers)\n",
    "#test_model.blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6752119-242c-41c5-a882-d202168b7a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model.to('cuda')\n",
    "test_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e39afbc4-9dd2-445f-8413-c8d0060f5c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b752ca-71d2-43ca-8435-39c9ce1e3ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8edf5071-1c16-4caa-a9b8-7e0c528589e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 196, 1024]), torch.Size([1, 1024, 14, 14]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_img = torch.randn((1, 3, 224, 224), device='cuda')\n",
    "\n",
    "out = test_model(ts_img)\n",
    "\n",
    "out2 = out.view(1, 14, 14, 1024) \n",
    "out2 = out2.permute(0, 3, 1, 2)\n",
    "out.shape, out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0df39b45-aeee-4600-8546-8646e0f9f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94efd997-2bbd-4c45-87a5-e734aab16605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef6eb0a1-52b0-46a7-a979-0249a92c4b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_conv = nn.Conv2d(1024, 3072, kernel_size=1)\n",
    "self_conv.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a2f38a0-d922-4c3b-a641-6a96ca4226dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1024, 3072, kernel_size=(1, 1), stride=(1, 1))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce4f86cc-8f9e-4ad3-a303-2ca23a0e35aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3072, 14, 14])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = self_conv(out2)\n",
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89731330-b6c0-493a-916b-3d0c312f0fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348bfa66-e111-4a14-ad74-a137e3f0c30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9c000-cdf7-46c7-800e-b1c0751d57a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ceebe-428a-4ea4-b0f8-5e24cc5edca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cc5b3-b65b-4d3f-8a6a-07758320bbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243e5a1-21e8-466c-b8ec-a3a915e1d70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca9f55-e0c6-488e-88ce-cc4717390709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "        h, w = self.size\n",
    "        \n",
    "        x = self.layers_before(x)\n",
    "        bs, hw, c = x.shape # bs, c, h, w\n",
    "        \n",
    "\n",
    "        # block 0\n",
    "        block0 = self.main_block_layer[0] \n",
    "        print(x.shape)\n",
    "        x = x.reshape(bs, h, w, c)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.norm(x) #block0.norm1(x)\n",
    "        x = block0.attn.qkv(x) #nn.Sequential( )\n",
    "        x = block0.attn.q_norm(x)\n",
    "        x = block0.attn.k_norm(x)\n",
    "        x = x.view(bs, 1024, 3, h, w).sum(dim=2)  # Sum over the new dimension\n",
    "        x = block0.attn.proj(x)\n",
    "        x = block0.attn.proj_drop(x)\n",
    "        \n",
    "        x = block0.ls1(x)\n",
    "        x = block0.drop_path1(x)\n",
    "        print(x.shape)\n",
    "        x = self.norm(x) #block0.norm2(x)\n",
    "\n",
    "        x = block0.mlp.fc1(x)\n",
    "        x = block0.mlp.act(x)\n",
    "        x = block0.mlp.drop1(x)\n",
    "        x = block0.mlp.norm(x)\n",
    "        x = block0.mlp.fc2(x)\n",
    "        x = block0.mlp.drop2(x)\n",
    "\n",
    "        x = block0.ls2(x)\n",
    "        x = block0.drop_path2drop_path2(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
