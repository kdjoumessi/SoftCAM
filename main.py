import os
import sys
import time
import random

import torch
import numpy as np
from torch.utils.tensorboard import SummaryWriter

from utils.func import *
from train import train, evaluate
from utils.metrics import Estimator
from data.builder import generate_dataset
from modules.builder import generate_model

def main():
    # load conf and paths files
    args = parse_config()
    cfg, cfg_paths = load_config(args)   
    cfg = data_path(cfg, cfg_paths)

    if cfg.train.network=='retfound':
        cfg.dset['data_dir'] = 'kaggle_data_224'
        cfg.data['input_size'] = 224
        cfg.train.batch_size = 16
        cfg.train.epochs = 300

    if cfg.base.test:
        print('########## test')
        cfg.base.test = True
        cfg.base.sample = 40
        cfg.train.epochs = 4
        cfg.train.batch_size = 2          
    else:
        if cfg.base.dataset=='Retinal_OCT':
            cfg.train.epochs = 30  
            #cfg.train.batch_size = 12
    
        if cfg.train.network == 'efficientnet_v2_m':
            #if cfg.base.dataset=='RSNA':
            cfg.train.batch_size = 8

    if cfg.base.dataset=='Fundus':
        if not cfg.data.binary:
            print('Multi-class Fundus')
            cfg.data.target = 'level'
            cfg.data.num_classes = 5
            cfg.data.threshold = 2
    elif cfg.base.dataset=='Retinal_OCT':
        if not cfg.data.binary:
            print('Multi-class OCT')
            cfg.data.target = 'level'
            cfg.data.num_classes = 4
            cfg.data.threshold = 2
            cfg.dset['train_csv'] = './csv_files/Retinal_OCT/multiclass/train.csv'
            cfg.dset['test_csv'] = './csv_files/Retinal_OCT/multiclass/test.csv'
            cfg.dset['val_csv'] = './csv_files/Retinal_OCT/multiclass/val.csv'

     
    # create folder
    cfg.dset.save_path = load_save_paths(cfg)
    save_path = cfg.dset.save_path 

    cfg.base.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  
    
    print('########### keys parameters ###########')  
    print('dataset dir: ', cfg.dset['data_dir'])
    print('root dataset dir: ', cfg.dset['root'])
    print('save path ', save_path)
    print('\ndevice ', cfg.base.device)
    print('Network: ', cfg.train.network)
    print('conv_cls: ', cfg.train.conv_cls)  
    print('reg l1: ', cfg.train.lambda_l1)    
    print('reg l2: ', cfg.train.lambda_l2)   
    print('###################### \n')
    
    if not os.path.exists(save_path):
        os.makedirs(save_path)
        #sys.exit(0)

    logger = SummaryWriter(save_path)    

    folders = ['configs', 'data', 'modules', 'utils']
    copy_file(folders, save_path)
    
    # print configuration
    if args.print_config:
        print_config({
            'BASE CONFIG': cfg.base,
            'DATA CONFIG': cfg.data,
            'TRAIN CONFIG': cfg.train
        })
    else:
        print_msg('LOADING CONFIG FILE: {}'.format(args.config))

    since = time.time()
    set_random_seed(cfg.base.random_seed)    
    model = generate_model(cfg)

    train_dataset, test_dataset, val_dataset = generate_dataset(cfg)

    estimator = Estimator(cfg)
    train(
        cfg=cfg,
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        estimator=estimator,
        logger=logger
    )

    ################## test ############
    ## Manual test 
    #save_path = './save_models'

    name = 'best_validation_weights'
    if cfg.data.binary: # only use for binary task to directly save the performance of the best model on the test set
        model_list = ['acc', 'auc', 'spec', 'sens', 'prec']
        model_name = ['Accuracy', 'AUC', 'Specificity', 'Sensitivity', 'Precision']
    else:
        model_list = ['acc', 'kappa'] #['acc'] 'loss'
        model_name = ['Accuracy', 'Kappa'] # ['Accuracy'] 'loss'
        
    #eval_best_models = multi_evaluate if cfg.train.multitask else evaluate 
        
    for i in range(len(model_list)):
        print('========================================')
        print(f'This is the performance of the final model base on the best {model_name[i]}')
        checkpoint = os.path.join(save_path, f'{name}_{model_list[i]}.pt')

        evaluate(cfg, model, checkpoint, val_dataset, estimator, type_ds='validation')
    
        print('')
        evaluate(cfg, model, checkpoint, test_dataset, estimator, type_ds='test')
        print('')

    time_elapsed = time.time() - since
    print('Training and evaluation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))

def set_random_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

if __name__ == '__main__':
    main()
